{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "R-3AH7ZCmZ5p",
        "HAwi3OYdM8CC",
        "Pfj-vhXHM1UK",
        "b85gKWYOMdD_",
        "__22u0PFMjYq"
      ],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ],
      "metadata": {
        "id": "ukK2xuHdBTgN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries and the datasets"
      ],
      "metadata": {
        "id": "66rfi56A_2C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets torch"
      ],
      "metadata": {
        "id": "FcC3n9bpI5gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import required libraries\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import set_seed\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, precision_recall_fscore_support"
      ],
      "metadata": {
        "id": "EiqEMWSt8lDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Define function to set the seed\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "#set_seed(42)"
      ],
      "metadata": {
        "id": "c84PEhQVkOxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Import the data and preprocess them\n",
        "\n",
        "conan = pd.read_csv('conan.csv')\n",
        "twitter = pd.read_csv('twitter.csv')\n",
        "\n",
        "## Create the combined dataset and drop rows with missing values in required columns\n",
        "data = pd.concat([conan, twitter], ignore_index=True)\n",
        "data = data.dropna(subset=['hate_speech', 'counter_speech','clarity','evidence', 'rebuttal', 'fairness'])\n",
        "## Ensure text columns are strings and integers\n",
        "data['hate_speech'] = data['hate_speech'].astype(str)\n",
        "data['counter_speech'] = data['counter_speech'].astype(str)\n",
        "data['clarity'] = data['clarity'].astype(int)\n",
        "data['evidence'] = data['evidence'].astype(int)\n",
        "data['rebuttal'] = data['rebuttal'].astype(int)\n",
        "data['fairness'] = data['fairness'].astype(int)\n",
        "\n",
        "## Preprocess data for the two separate datasets\n",
        "conan = conan.dropna(subset=['hate_speech', 'counter_speech','clarity','evidence', 'rebuttal', 'fairness'])\n",
        "conan['hate_speech'] = conan['hate_speech'].astype(str)\n",
        "conan['counter_speech'] = conan['counter_speech'].astype(str)\n",
        "conan['clarity'] = conan['clarity'].astype(int)\n",
        "conan['evidence'] = conan['evidence'].astype(int)\n",
        "conan['rebuttal'] = conan['rebuttal'].astype(int)\n",
        "conan['fairness'] = conan['fairness'].astype(int)\n",
        "\n",
        "twitter = twitter.dropna(subset=['hate_speech', 'counter_speech','clarity','evidence', 'rebuttal', 'fairness'])\n",
        "twitter['hate_speech'] = twitter['hate_speech'].astype(str)\n",
        "twitter['counter_speech'] = twitter['counter_speech'].astype(str)\n",
        "twitter['clarity'] = twitter['clarity'].astype(int)\n",
        "twitter['evidence'] = twitter['evidence'].astype(int)\n",
        "twitter['rebuttal'] = twitter['rebuttal'].astype(int)\n",
        "twitter['fairness'] = twitter['fairness'].astype(int)"
      ],
      "metadata": {
        "id": "cYmWUdZWG2RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models with only CS embeddings (Bert_CS)"
      ],
      "metadata": {
        "id": "NkQcYB7A-Qt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary variables"
      ],
      "metadata": {
        "id": "dY3nm6v-ugrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Define the required funtions for preprocessing, splitting data, computing metrics, training and testing\n",
        "\n",
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    # Only use counter_speech for input\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, test_size=0.2, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation, and testing, with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train+val and test sets\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    # Further split train+val into train and validation sets\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs using only counter_speech\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        'clf_report': clf_report,\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),  # Enable mixed precision training if GPU is available\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for binary classification\n",
        "effectiveness_dimensions = {\n",
        "    \"emotional_appeal\": 2,  # Binary classification (0-1)\n",
        "    \"audience_adaptation\": 2,  # Binary classification (0-1)\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    #Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}_test\",\n",
        "        per_device_eval_batch_size=16,\n",
        "        do_eval=True,\n",
        "    )\n",
        "\n",
        "    test_trainer = Trainer(\n",
        "        model=trained_model,\n",
        "        args=test_args,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    test_results = test_trainer.evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_bert_cs_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "D0sXt39ZudOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label variables"
      ],
      "metadata": {
        "id": "yLrCA2wo_R7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    # Only using counter_speech for classification\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, test_size=0.2, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation, and testing, with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train+val and test sets\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    # Further split train+val into train and validation sets\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs only counter_speech\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        def adjust_labels(example):\n",
        "            example['labels'] = torch.tensor(int(example['labels']) - 1)  # Subtract 1 from labels\n",
        "            return example\n",
        "\n",
        "        train_dataset = train_dataset.map(adjust_labels)\n",
        "        val_dataset = val_dataset.map(adjust_labels)\n",
        "        test_dataset = test_dataset.map(adjust_labels)\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"clf_report\": clf_report\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        no_cuda=False\n",
        "        #fp16=torch.cuda.is_available(), # Enable mixed precision training if GPU is available\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_results_cs.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for multi-label classification\n",
        "effectiveness_dimensions = {\n",
        "    \"clarity\": 3,  # multi-label classification (1-3)\n",
        "    \"evidence\": 3,\n",
        "    'rebuttal': 3,\n",
        "    'fairness': 3,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_bert_cs_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "eQHQ4mUS_WTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models with CS + HS embeddings (Bert_CS_HS)"
      ],
      "metadata": {
        "id": "VZK5cI6HAIIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary variables"
      ],
      "metadata": {
        "id": "R-3AH7ZCmZ5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['hate_speech'].tolist(),\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, test_size=0.2, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation, and testing, with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['hate_speech', 'counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train+val and test sets\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    # Further split train+val into train and validation sets\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['hate_speech'],\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        'clf_report': clf_report,\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for binary classification\n",
        "effectiveness_dimensions = {\n",
        "    \"emotional_appeal\": 2,\n",
        "    \"audience_adaptation\": 2,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")"
      ],
      "metadata": {
        "id": "aJHoZxAzjKm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label variables"
      ],
      "metadata": {
        "id": "Lw36SCrzmdHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['hate_speech'].tolist(),\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, test_size=0.2, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation, and testing, with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['hate_speech', 'counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train+val and test sets\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    # Further split train+val into train and validation sets\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['hate_speech'],\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        def adjust_labels(example):\n",
        "            example['labels'] = torch.tensor(int(example['labels']) - 1)  # Subtract 1 from labels\n",
        "            return example\n",
        "\n",
        "        train_dataset = train_dataset.map(adjust_labels)\n",
        "        val_dataset = val_dataset.map(adjust_labels)\n",
        "        test_dataset = test_dataset.map(adjust_labels)\n",
        "\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"clf_report\": clf_report\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        no_cuda=False\n",
        "        #fp16=torch.cuda.is_available(), # Enable mixed precision training if GPU is available\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for multi-label classification\n",
        "effectiveness_dimensions = {\n",
        "    \"clarity\": 3,  # multi-label classification (1-3)\n",
        "    \"evidence\": 3,\n",
        "    'rebuttal': 3,\n",
        "    'fairness': 3,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(data, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "XGJz5O2DmtaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation setting"
      ],
      "metadata": {
        "id": "LnRJ9Q37A_CG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary CS cross-validation"
      ],
      "metadata": {
        "id": "hvw3jZakl6a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train on Twitter & test on CONAN**"
      ],
      "metadata": {
        "id": "fnEkweVJsCt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    # Only use counter_speech for input\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(twitter_df, conan_df, target_col, tokenizer, max_length=128, val_size=0.1, random_state=1, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation (from Twitter), and testing (from Conan), with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns in both datasets\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in twitter_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Twitter dataset: {', '.join(set(required_columns) - set(twitter_df.columns))}\")\n",
        "    if not all(col in conan_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Conan dataset: {', '.join(set(required_columns) - set(conan_df.columns))}\")\n",
        "\n",
        "    # Split Twitter dataset into train and validation sets\n",
        "    train_df, val_df = train_test_split(twitter_df, test_size=val_size, random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs using only counter_speech\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(conan_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        'clf_report': clf_report,\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=1):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for binary classification\n",
        "effectiveness_dimensions = {\n",
        "    \"emotional_appeal\": 2,  # Binary classification (0-1)\n",
        "    \"audience_adaptation\": 2,  # Binary classification (0-1)\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=True\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=False\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_new.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "2LzeXjhVl_JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train on CONAN & test on Twitter**"
      ],
      "metadata": {
        "id": "d3w6I1TFr8gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    # Only use counter_speech for input\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(twitter, conan, target_col, tokenizer, max_length=128, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation (from Twitter), and testing (from Conan), with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns in both datasets\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in twitter.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Twitter dataset: {', '.join(set(required_columns) - set(twitter.columns))}\")\n",
        "    if not all(col in conan.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Conan dataset: {', '.join(set(required_columns) - set(conan.columns))}\")\n",
        "\n",
        "    # Split conan dataset into train and validation sets\n",
        "    train_df, val_df = train_test_split(conan, test_size=val_size, random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs using only counter_speech\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(twitter)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        'clf_report': clf_report,\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for binary classification\n",
        "effectiveness_dimensions = {\n",
        "    \"emotional_appeal\": 2,  # Binary classification (0-1)\n",
        "    \"audience_adaptation\": 2,  # Binary classification (0-1)\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter=twitter,\n",
        "            conan=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=True\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter=twitter,\n",
        "            conan=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=False\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "79o2QZnpr49P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary HS + CS cross-validation"
      ],
      "metadata": {
        "id": "cjlTvGEWmAso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In lines 2017 and 220, set the name of the dataset on which you want to train and test."
      ],
      "metadata": {
        "id": "rrjURVKlCYLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['hate_speech'].tolist(),\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares training and validation datasets from a single dataframe.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['hate_speech', 'counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train and validation sets\n",
        "    train_df, val_df = train_test_split(df, test_size=val_size, random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['hate_speech'],\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def prepare_test_dataset(df, target_col, tokenizer, max_length=128, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares the test dataset from a single dataframe.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['hate_speech', 'counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['hate_speech'],\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframe to Hugging Face Dataset\n",
        "    test_dataset = Dataset.from_pandas(df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor(int(x[\"labels\"]))})\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    test_dataset = test_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        'clf_report': clf_report,\n",
        "    }\n",
        "\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        fp16=torch.cuda.is_available(),\n",
        "    )\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example dimensions for binary classification\n",
        "effectiveness_dimensions = {\n",
        "    \"emotional_appeal\": 2,  # Binary classification (0-1)\n",
        "    \"audience_adaptation\": 2,  # Binary classification (0-1)\n",
        "}\n",
        "\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare training and validation datasets from Conan\n",
        "    train_dataset, val_dataset = prepare_datasets(conan, target_col=dim, tokenizer=tokenizer, is_binary=(num_labels == 2))\n",
        "\n",
        "    # Prepare test dataset from Twitter\n",
        "    test_dataset = prepare_test_dataset(twitter, target_col=dim, tokenizer=tokenizer, is_binary=(num_labels == 2))\n",
        "\n",
        "    # Initialize the model\n",
        "    model = initialize_model(num_labels, is_binary=(num_labels == 2))\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the Twitter test set\n",
        "    print(\"\\nEvaluating on the conan test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_conan_test_results.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "MrhuMq6T74iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label CS cross-validation"
      ],
      "metadata": {
        "id": "-ouFrVqTQqiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train on Twitter & test on CONAN**"
      ],
      "metadata": {
        "id": "rsra13mqDu-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(twitter_df, conan_df, target_col, tokenizer, max_length=128, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training and validation.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in twitter_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Twitter dataset: {', '.join(set(required_columns) - set(twitter_df.columns))}\")\n",
        "    if not all(col in conan_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Conan dataset: {', '.join(set(required_columns) - set(conan_df.columns))}\")\n",
        "\n",
        "    # Split Twitter dataset into train and validation sets\n",
        "    train_df, val_df = train_test_split(twitter_df, test_size=val_size, random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(conan_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        def adjust_labels(example):\n",
        "            example['labels'] = torch.tensor(int(example['labels']) - 1)  # Subtract 1 from labels\n",
        "            return example\n",
        "\n",
        "        train_dataset = train_dataset.map(adjust_labels)\n",
        "        val_dataset = val_dataset.map(adjust_labels)\n",
        "        test_dataset = test_dataset.map(adjust_labels)\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"clf_report\": clf_report\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        #metric_for_best_model=\"accuracy\",\n",
        "        no_cuda=False\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_results_cs.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for multi-label classification\n",
        "effectiveness_dimensions = {\n",
        "    \"clarity\": 3,  # multi-label classification (1-3)\n",
        "    \"evidence\": 3,\n",
        "    'rebuttal': 3,\n",
        "    'fairness': 3,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=True\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=False\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_cs_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "-GVCCqcXRSbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train on CONAN & test on Twitter**"
      ],
      "metadata": {
        "id": "ExxD3DJGDmha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(twitter_df, conan_df, target_col, tokenizer, max_length=128, val_size=0.1, random_state=3, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training and validation from `twitter_df` and testing from `conan_df`.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['counter_speech', target_col]\n",
        "    if not all(col in twitter_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Twitter dataset: {', '.join(set(required_columns) - set(twitter_df.columns))}\")\n",
        "    if not all(col in conan_df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns in Conan dataset: {', '.join(set(required_columns) - set(conan_df.columns))}\")\n",
        "\n",
        "    # Split CONAN dataset into train and validation sets\n",
        "    train_df, val_df = train_test_split(conan_df, test_size=val_size, random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(twitter_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        def adjust_labels(example):\n",
        "            example['labels'] = torch.tensor(int(example['labels']) - 1)  # Subtract 1 from labels\n",
        "            return example\n",
        "\n",
        "        train_dataset = train_dataset.map(adjust_labels)\n",
        "        val_dataset = val_dataset.map(adjust_labels)\n",
        "        test_dataset = test_dataset.map(adjust_labels)\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"clf_report\": clf_report\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=3):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        #metric_for_best_model=\"accuracy\",\n",
        "        no_cuda=False\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_results_cs.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "# Example dimensions for multi-label classification\n",
        "effectiveness_dimensions = {\n",
        "    \"clarity\": 3,  # multi-label classification (1-3)\n",
        "    \"evidence\": 3,\n",
        "    'rebuttal': 3,\n",
        "    'fairness': 3,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets based on whether it's binary or multi-class classification\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=True\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
        "            twitter_df=twitter,\n",
        "            conan_df=conan,\n",
        "            target_col=dim,\n",
        "            tokenizer=tokenizer,\n",
        "            is_binary=False\n",
        "        )\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    print(\"\\nEvaluating on the test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_cs_short.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "2aFFVB-nV54r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-label HS + CS cross-validation"
      ],
      "metadata": {
        "id": "WhvpKDnzO2Ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From line 184-191, set the dataset for training and testing."
      ],
      "metadata": {
        "id": "_80mfyKIEx8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data, tokenizer, max_length, target_col):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text data and prepares labels for the given target column.\n",
        "    \"\"\"\n",
        "    inputs = tokenizer(\n",
        "        data['hate_speech'].tolist(),\n",
        "        data['counter_speech'].tolist(),\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    labels = torch.tensor(data[target_col].values)\n",
        "    return inputs, labels\n",
        "\n",
        "def prepare_datasets(df, target_col, tokenizer, max_length=128, test_size=0.2, val_size=0.1, random_state=0, is_binary=False):\n",
        "    \"\"\"\n",
        "    Prepares datasets for training, validation, and testing, with handling for binary and multi-class labels.\n",
        "    \"\"\"\n",
        "    # Check required columns\n",
        "    required_columns = ['hate_speech', 'counter_speech', target_col]\n",
        "    if not all(col in df.columns for col in required_columns):\n",
        "        raise ValueError(f\"Missing required columns: {', '.join(set(required_columns) - set(df.columns))}\")\n",
        "\n",
        "    # Split data into train+val and test sets\n",
        "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    # Further split train+val into train and validation sets\n",
        "    train_df, val_df = train_test_split(train_val_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
        "\n",
        "    # Tokenize inputs\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples['hate_speech'],\n",
        "            examples['counter_speech'],\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "    # Convert dataframes to Hugging Face Datasets\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    val_dataset = Dataset.from_pandas(val_df)\n",
        "    test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "    # Apply tokenization\n",
        "    train_dataset = train_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    val_dataset = val_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "    test_dataset = test_dataset.map(tokenize_function, batched=True, load_from_cache_file=False)\n",
        "\n",
        "    # Rename target column to \"labels\"\n",
        "    train_dataset = train_dataset.rename_column(target_col, \"labels\")\n",
        "    val_dataset = val_dataset.rename_column(target_col, \"labels\")\n",
        "    test_dataset = test_dataset.rename_column(target_col, \"labels\")\n",
        "\n",
        "\n",
        "    # Adjust labels\n",
        "    if is_binary:\n",
        "        train_dataset = train_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        val_dataset = val_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "        test_dataset = test_dataset.map(lambda x: {\"labels\": torch.tensor([float(x[\"labels\"])])})\n",
        "    else:\n",
        "        def adjust_labels(example):\n",
        "            example['labels'] = torch.tensor(int(example['labels']) - 1)  # Subtract 1 from labels\n",
        "            return example\n",
        "\n",
        "        train_dataset = train_dataset.map(adjust_labels)\n",
        "        val_dataset = val_dataset.map(adjust_labels)\n",
        "        test_dataset = test_dataset.map(adjust_labels)\n",
        "\n",
        "\n",
        "    # Remove unnecessary columns\n",
        "    train_dataset = train_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    val_dataset = val_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "    test_dataset = test_dataset.remove_columns(['hate_speech', 'counter_speech'])\n",
        "\n",
        "    # Set the format for PyTorch\n",
        "    train_dataset.set_format(\"torch\")\n",
        "    val_dataset.set_format(\"torch\")\n",
        "    test_dataset.set_format(\"torch\")\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "def initialize_model(num_labels, is_binary=False):\n",
        "    \"\"\"\n",
        "    Initializes a BERT model for binary or multi-class classification.\n",
        "    \"\"\"\n",
        "    if is_binary:\n",
        "        num_labels = 1  # Single output for sigmoid activation\n",
        "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "    return model\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy, F1 score, precision, and recall for the given predictions.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    labels = labels.astype(int)\n",
        "\n",
        "    if logits.shape[1] == 1:  # Binary classification with sigmoid\n",
        "        preds = (torch.sigmoid(torch.FloatTensor(logits)) > 0.5).long()\n",
        "    else:  # Multi-class classification with softmax\n",
        "        probs = torch.softmax(torch.FloatTensor(logits), dim=1)\n",
        "        preds = np.argmax(probs, axis=-1)\n",
        "    preds = preds.cpu().detach().numpy()\n",
        "    # Calculate metrics\n",
        "    clf_report = classification_report(labels, preds, output_dict=True)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    precision = precision_score(labels, preds, average='weighted')\n",
        "    recall = recall_score(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"clf_report\": clf_report\n",
        "    }\n",
        "\n",
        "def train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer, seed=0):\n",
        "    \"\"\"\n",
        "    Trains the BERT model and saves the best model and tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{dim}\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",  # Evaluation strategy set to epoch\n",
        "        save_strategy=\"epoch\",  # Save strategy also set to epoch\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        no_cuda=False\n",
        "\n",
        "    )\n",
        "\n",
        "    # Define Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model and tokenizer\n",
        "    model.save_pretrained(f\"./models/{dim}\")\n",
        "    tokenizer.save_pretrained(f\"./models/{dim}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    results = trainer.evaluate()\n",
        "    with open(f\"./results/{dim}_eval_results.txt\", \"w\") as f:\n",
        "        for key, value in results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "    return trainer.model\n",
        "\n",
        "\n",
        "# Example dimensions for multi-label classification\n",
        "effectiveness_dimensions = {\n",
        "    \"clarity\": 3,  # multi-label classification (1-3)\n",
        "    \"evidence\": 3,\n",
        "    'rebuttal': 3,\n",
        "    'fairness': 3,\n",
        "}\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Iterate through dimensions and train models\n",
        "for dim, num_labels in effectiveness_dimensions.items():\n",
        "    print(f\"\\nTraining model for {dim} with {num_labels} labels...\\n\")\n",
        "\n",
        "    # Prepare datasets for training using conan data and testing using twitter data\n",
        "    if num_labels == 2:  # Binary classification\n",
        "        train_dataset, val_dataset, _ = prepare_datasets(conan, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        _, _, test_dataset = prepare_datasets(twitter, target_col=dim, tokenizer=tokenizer, is_binary=True)\n",
        "        model = initialize_model(num_labels, is_binary=True)\n",
        "    else:  # Multi-class classification\n",
        "        train_dataset, val_dataset, _ = prepare_datasets(conan, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        _, _, test_dataset = prepare_datasets(twitter, target_col=dim, tokenizer=tokenizer, is_binary=False)\n",
        "        model = initialize_model(num_labels, is_binary=False)\n",
        "\n",
        "    # Train the model\n",
        "    trained_model = train_model(dim, num_labels, train_dataset, val_dataset, model, tokenizer)\n",
        "\n",
        "    # Evaluate on the twitter test set\n",
        "    print(\"\\nEvaluating on the conan test set...\")\n",
        "    test_results = Trainer(\n",
        "        model=trained_model,\n",
        "        args=TrainingArguments(output_dir=f\"./results/{dim}_test\"),\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    ).evaluate(test_dataset)\n",
        "    print(test_results)\n",
        "\n",
        "    # Save test set results\n",
        "    with open(f\"./results/{dim}_t_new.txt\", \"w\") as f:\n",
        "        for key, value in test_results.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n"
      ],
      "metadata": {
        "id": "y0W5CsdpEm96"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}